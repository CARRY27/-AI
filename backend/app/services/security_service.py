"""
å®‰å…¨æœåŠ¡
æ•æ„Ÿè¯æ£€æµ‹ã€å†…å®¹å®¡æ ¸
"""

import re
from typing import Dict, List, Tuple
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.review import SensitiveWordLog


class SecurityService:
    """å®‰å…¨æœåŠ¡"""
    
    # æ•æ„Ÿè¯åº“ï¼ˆç¤ºä¾‹ï¼Œå®é™…åº”ä»é…ç½®æˆ–æ•°æ®åº“åŠ è½½ï¼‰
    SENSITIVE_WORDS = {
        "political": ["æ”¿æ²»æ•æ„Ÿè¯1", "æ”¿æ²»æ•æ„Ÿè¯2"],  # æ”¿æ²»ç±»
        "discrimination": ["æ­§è§†è¯1", "æ­§è§†è¯2"],  # æ­§è§†ç±»
        "adult": ["æ¶‰é»„è¯1", "æ¶‰é»„è¯2"],  # æˆäººå†…å®¹
        "violence": ["æš´åŠ›è¯1", "æš´åŠ›è¯2"],  # æš´åŠ›ç±»
        "commercial_secret": ["æœºå¯†", "å†…éƒ¨èµ„æ–™", "ç»å¯†"],  # å•†ä¸šæœºå¯†
    }
    
    # é£é™©ç­‰çº§
    RISK_LEVELS = {
        "political": "critical",
        "discrimination": "high",
        "adult": "critical",
        "violence": "high",
        "commercial_secret": "medium",
    }
    
    def __init__(self, db: AsyncSession = None):
        self.db = db
    
    def check_sensitive_content(self, text: str) -> Dict:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«æ•æ„Ÿå†…å®¹
        
        Returns:
            {
                "has_sensitive": bool,
                "risk_level": str,
                "detected_words": List[str],
                "categories": List[str],
                "should_block": bool
            }
        """
        detected_words = []
        categories = []
        max_risk_level = "low"
        
        for category, words in self.SENSITIVE_WORDS.items():
            for word in words:
                if word in text:
                    detected_words.append(word)
                    if category not in categories:
                        categories.append(category)
                    
                    # æ›´æ–°é£é™©ç­‰çº§
                    current_risk = self.RISK_LEVELS.get(category, "low")
                    if self._compare_risk_level(current_risk, max_risk_level) > 0:
                        max_risk_level = current_risk
        
        has_sensitive = len(detected_words) > 0
        should_block = max_risk_level in ["critical", "high"]
        
        return {
            "has_sensitive": has_sensitive,
            "risk_level": max_risk_level,
            "detected_words": detected_words,
            "categories": categories,
            "should_block": should_block
        }
    
    def _compare_risk_level(self, level1: str, level2: str) -> int:
        """æ¯”è¾ƒé£é™©ç­‰çº§
        
        Returns:
            1: level1 > level2
            0: level1 == level2
            -1: level1 < level2
        """
        levels = {"low": 1, "medium": 2, "high": 3, "critical": 4}
        return levels.get(level1, 0) - levels.get(level2, 0)
    
    async def log_sensitive_detection(
        self,
        content_type: str,
        text: str,
        detection_result: Dict,
        message_id: int = None,
        file_id: int = None
    ):
        """è®°å½•æ•æ„Ÿè¯æ£€æµ‹æ—¥å¿—"""
        if not self.db or not detection_result["has_sensitive"]:
            return
        
        import json
        
        log = SensitiveWordLog(
            message_id=message_id,
            file_id=file_id,
            content_type=content_type,
            detected_words=json.dumps(detection_result["detected_words"], ensure_ascii=False),
            risk_level=detection_result["risk_level"],
            original_text=text[:500],  # åªä¿å­˜å‰500å­—ç¬¦
            is_blocked=detection_result["should_block"]
        )
        
        self.db.add(log)
        await self.db.commit()
    
    def calculate_confidence(self, similarities: List[float], threshold: float = 0.75) -> float:
        """è®¡ç®—ç­”æ¡ˆç½®ä¿¡åº¦
        
        åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦
        
        Args:
            similarities: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç›¸ä¼¼åº¦åˆ—è¡¨
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•° (0.0-1.0)
        """
        if not similarities:
            return 0.0
        
        # è¿‡æ»¤ä½äºé˜ˆå€¼çš„ç»“æœ
        valid_sims = [s for s in similarities if s >= threshold]
        
        if not valid_sims:
            return 0.0
        
        # åŠ æƒå¹³å‡ï¼ˆè¶Šé å‰æƒé‡è¶Šé«˜ï¼‰
        weights = [1.0 / (i + 1) for i in range(len(valid_sims))]
        weighted_sum = sum(s * w for s, w in zip(valid_sims, weights))
        weight_sum = sum(weights)
        
        confidence = weighted_sum / weight_sum
        
        # æ ¹æ®å¬å›æ•°é‡è°ƒæ•´ç½®ä¿¡åº¦
        if len(valid_sims) < 3:
            confidence *= 0.8  # å¬å›æ•°é‡å°‘ï¼Œé™ä½ç½®ä¿¡åº¦
        
        return min(confidence, 1.0)
    
    def add_disclaimer(self, answer: str) -> str:
        """ä¸ºAIå›ç­”æ·»åŠ å…è´£å£°æ˜"""
        disclaimer = "\n\n---\nğŸ’¡ **å…è´£å£°æ˜**ï¼šä»¥ä¸Šå›ç­”ç”± AI åŸºäºä¼ä¸šçŸ¥è¯†åº“ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚å¦‚æœ‰ç–‘é—®è¯·å’¨è¯¢ç›¸å…³éƒ¨é—¨æˆ–æŸ¥é˜…åŸå§‹æ–‡æ¡£ã€‚"
        return answer + disclaimer

