"""
æ¨¡å‹è°ƒåº¦å™¨ (Model Orchestrator)
æ”¯æŒå¤šæ¨¡å‹ã€fallbackæœºåˆ¶ã€rate limiter
"""

from typing import Dict, List, Optional, Any, AsyncGenerator
from enum import Enum
import time
import asyncio
from datetime import datetime, timedelta

import openai
from app.config import settings
from app.services.cache_service import cache_service


class ModelProvider(str, Enum):
    """æ¨¡å‹æä¾›å•†"""
    OPENAI = "openai"
    AZURE_OPENAI = "azure_openai"
    CLAUDE = "claude"
    TONGYI = "tongyi"  # é€šä¹‰åƒé—®
    OLLAMA = "ollama"


class TaskType(str, Enum):
    """ä»»åŠ¡ç±»å‹"""
    QA = "qa"  # é—®ç­”
    SUMMARIZATION = "summarization"  # æ‘˜è¦
    EXTRACTION = "extraction"  # æå–
    TRANSLATION = "translation"  # ç¿»è¯‘
    GENERAL = "general"  # é€šç”¨


class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    
    def __init__(
        self,
        provider: ModelProvider,
        model_name: str,
        api_key: str,
        api_base: Optional[str] = None,
        max_tokens: int = 2000,
        temperature: float = 0.1,
        priority: int = 1,  # ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜
        rate_limit_per_minute: int = 60,
        timeout: int = 30
    ):
        self.provider = provider
        self.model_name = model_name
        self.api_key = api_key
        self.api_base = api_base
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.priority = priority
        self.rate_limit_per_minute = rate_limit_per_minute
        self.timeout = timeout
        
        # è¿è¡Œæ—¶çŠ¶æ€
        self.is_available = True
        self.error_count = 0
        self.last_error_time = None


class ModelOrchestrator:
    """æ¨¡å‹è°ƒåº¦å™¨"""
    
    def __init__(self):
        self.models: Dict[TaskType, List[ModelConfig]] = {}
        self.call_history: Dict[str, List[float]] = {}  # ç”¨äºrate limiting
        
        # åˆå§‹åŒ–é»˜è®¤æ¨¡å‹é…ç½®
        self._init_default_models()
    
    def _init_default_models(self):
        """åˆå§‹åŒ–é»˜è®¤æ¨¡å‹é…ç½®"""
        
        # OpenAI GPT-4 - é«˜è´¨é‡é—®ç­”
        if settings.OPENAI_API_KEY:
            gpt4_config = ModelConfig(
                provider=ModelProvider.OPENAI,
                model_name="gpt-4o-mini",
                api_key=settings.OPENAI_API_KEY,
                api_base=settings.OPENAI_API_BASE,
                max_tokens=2000,
                temperature=0.1,
                priority=1,
                rate_limit_per_minute=60
            )
            
            # GPT-4ç”¨äºæ‰€æœ‰ä»»åŠ¡ç±»å‹
            for task_type in TaskType:
                if task_type not in self.models:
                    self.models[task_type] = []
                self.models[task_type].append(gpt4_config)
        
        # é€šä¹‰åƒé—® - ä¸­æ–‡ä¼˜åŒ–
        if settings.TONGYI_API_KEY:
            tongyi_config = ModelConfig(
                provider=ModelProvider.TONGYI,
                model_name=settings.TONGYI_MODEL,
                api_key=settings.TONGYI_API_KEY,
                max_tokens=2000,
                temperature=0.1,
                priority=1,  # è®¾ç½®ä¸ºé«˜ä¼˜å…ˆçº§ï¼Œä¼˜å…ˆä½¿ç”¨é€šä¹‰åƒé—®
                rate_limit_per_minute=60
            )
            
            # é€šä¹‰åƒé—®ç”¨äºæ‰€æœ‰ä»»åŠ¡ç±»å‹
            for task_type in TaskType:
                if task_type not in self.models:
                    self.models[task_type] = []
                # æ’å…¥åˆ°åˆ—è¡¨å¼€å¤´ï¼Œä½¿å…¶ä¼˜å…ˆäºå…¶ä»–æ¨¡å‹
                self.models[task_type].insert(0, tongyi_config)
        
        # å¯ä»¥æ·»åŠ æ›´å¤šæ¨¡å‹é…ç½®
        # ä¾‹å¦‚ï¼šClaude for summarization, etc.
    
    def register_model(self, task_type: TaskType, model_config: ModelConfig):
        """æ³¨å†Œæ–°æ¨¡å‹"""
        if task_type not in self.models:
            self.models[task_type] = []
        
        self.models[task_type].append(model_config)
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        self.models[task_type].sort(key=lambda x: x.priority)
    
    def _check_rate_limit(self, model_key: str, rate_limit: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…è¿‡é€Ÿç‡é™åˆ¶"""
        now = time.time()
        minute_ago = now - 60
        
        # æ¸…ç†æ—§è®°å½•
        if model_key in self.call_history:
            self.call_history[model_key] = [
                t for t in self.call_history[model_key] if t > minute_ago
            ]
        else:
            self.call_history[model_key] = []
        
        # æ£€æŸ¥æ˜¯å¦è¶…é™
        if len(self.call_history[model_key]) >= rate_limit:
            return False
        
        # è®°å½•æœ¬æ¬¡è°ƒç”¨
        self.call_history[model_key].append(now)
        return True
    
    def _mark_model_error(self, model_config: ModelConfig):
        """æ ‡è®°æ¨¡å‹é”™è¯¯"""
        model_config.error_count += 1
        model_config.last_error_time = datetime.utcnow()
        
        # å¦‚æœè¿ç»­é”™è¯¯è¶…è¿‡3æ¬¡ï¼Œæš‚æ—¶æ ‡è®°ä¸ºä¸å¯ç”¨
        if model_config.error_count >= 3:
            model_config.is_available = False
            print(f"âš ï¸ æ¨¡å‹ {model_config.model_name} æš‚æ—¶ä¸å¯ç”¨")
    
    def _recover_model(self, model_config: ModelConfig):
        """æ¢å¤æ¨¡å‹å¯ç”¨æ€§"""
        if not model_config.is_available and model_config.last_error_time:
            # å¦‚æœè·ç¦»ä¸Šæ¬¡é”™è¯¯è¶…è¿‡5åˆ†é’Ÿï¼Œå°è¯•æ¢å¤
            time_since_error = datetime.utcnow() - model_config.last_error_time
            if time_since_error > timedelta(minutes=5):
                model_config.is_available = True
                model_config.error_count = 0
                print(f"âœ… æ¨¡å‹ {model_config.model_name} å·²æ¢å¤")
    
    def _select_model(self, task_type: TaskType) -> Optional[ModelConfig]:
        """é€‰æ‹©åˆé€‚çš„æ¨¡å‹"""
        if task_type not in self.models or not self.models[task_type]:
            return None
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•é€‰æ‹©å¯ç”¨æ¨¡å‹
        for model_config in self.models[task_type]:
            # å°è¯•æ¢å¤æ¨¡å‹
            self._recover_model(model_config)
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
            if not model_config.is_available:
                continue
            
            # æ£€æŸ¥é€Ÿç‡é™åˆ¶
            model_key = f"{model_config.provider}:{model_config.model_name}"
            if not self._check_rate_limit(model_key, model_config.rate_limit_per_minute):
                print(f"âš ï¸ æ¨¡å‹ {model_config.model_name} è¾¾åˆ°é€Ÿç‡é™åˆ¶")
                continue
            
            return model_config
        
        return None
    
    async def generate(
        self,
        messages: List[Dict[str, str]],
        task_type: TaskType = TaskType.GENERAL,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        fallback: bool = True
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ˆå¸¦fallbackï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            task_type: ä»»åŠ¡ç±»å‹
            temperature: æ¸©åº¦å‚æ•°ï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰
            fallback: æ˜¯å¦å¯ç”¨fallback
        
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        
        attempted_models = []
        
        while True:
            # é€‰æ‹©æ¨¡å‹
            model_config = self._select_model(task_type)
            
            if not model_config:
                if attempted_models:
                    raise Exception(f"æ‰€æœ‰æ¨¡å‹éƒ½ä¸å¯ç”¨ã€‚å·²å°è¯•: {attempted_models}")
                else:
                    raise Exception(f"æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ç”¨äºä»»åŠ¡ç±»å‹: {task_type}")
            
            # é¿å…é‡å¤å°è¯•åŒä¸€æ¨¡å‹
            model_key = f"{model_config.provider}:{model_config.model_name}"
            if model_key in attempted_models:
                if fallback and len(attempted_models) < len(self.models.get(task_type, [])):
                    continue
                else:
                    raise Exception(f"æ‰€æœ‰fallbackæ¨¡å‹éƒ½å·²å°è¯•å¤±è´¥: {attempted_models}")
            
            attempted_models.append(model_key)
            
            try:
                print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_config.model_name}")
                
                # æ ¹æ®providerè°ƒç”¨ä¸åŒçš„API
                if model_config.provider == ModelProvider.OPENAI:
                    result = await self._call_openai(
                        model_config=model_config,
                        messages=messages,
                        temperature=temperature or model_config.temperature,
                        max_tokens=max_tokens or model_config.max_tokens
                    )
                    
                elif model_config.provider == ModelProvider.AZURE_OPENAI:
                    result = await self._call_azure_openai(
                        model_config=model_config,
                        messages=messages,
                        temperature=temperature or model_config.temperature,
                        max_tokens=max_tokens or model_config.max_tokens
                    )
                    
                elif model_config.provider == ModelProvider.OLLAMA:
                    result = await self._call_ollama(
                        model_config=model_config,
                        messages=messages,
                        temperature=temperature or model_config.temperature,
                        max_tokens=max_tokens or model_config.max_tokens
                    )
                    
                elif model_config.provider == ModelProvider.TONGYI:
                    result = await self._call_tongyi(
                        model_config=model_config,
                        messages=messages,
                        temperature=temperature or model_config.temperature,
                        max_tokens=max_tokens or model_config.max_tokens
                    )
                    
                else:
                    raise Exception(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {model_config.provider}")
                
                # æˆåŠŸï¼Œé‡ç½®é”™è¯¯è®¡æ•°
                model_config.error_count = 0
                
                return result
                
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {model_config.model_name} è°ƒç”¨å¤±è´¥: {str(e)}")
                
                # æ ‡è®°é”™è¯¯
                self._mark_model_error(model_config)
                
                # å¦‚æœä¸å¯ç”¨fallbackï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
                if not fallback:
                    raise
                
                # ç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹
                continue
    
    async def stream_generate(
        self,
        messages: List[Dict[str, str]],
        task_type: TaskType = TaskType.GENERAL,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆæ–‡æœ¬
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            task_type: ä»»åŠ¡ç±»å‹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
        
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        
        # é€‰æ‹©æ¨¡å‹
        model_config = self._select_model(task_type)
        
        if not model_config:
            raise Exception(f"æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ç”¨äºä»»åŠ¡ç±»å‹: {task_type}")
        
        try:
            print(f"ğŸ¤– æµå¼ä½¿ç”¨æ¨¡å‹: {model_config.model_name}")
            
            if model_config.provider == ModelProvider.OPENAI:
                async for chunk in self._stream_openai(
                    model_config=model_config,
                    messages=messages,
                    temperature=temperature or model_config.temperature,
                    max_tokens=max_tokens or model_config.max_tokens
                ):
                    yield chunk
            
            elif model_config.provider == ModelProvider.TONGYI:
                async for chunk in self._stream_tongyi(
                    model_config=model_config,
                    messages=messages,
                    temperature=temperature or model_config.temperature,
                    max_tokens=max_tokens or model_config.max_tokens
                ):
                    yield chunk
            
            else:
                raise Exception(f"æ¨¡å‹ {model_config.provider} ä¸æ”¯æŒæµå¼è¾“å‡º")
            
            # æˆåŠŸï¼Œé‡ç½®é”™è¯¯è®¡æ•°
            model_config.error_count = 0
            
        except Exception as e:
            print(f"âŒ æµå¼ç”Ÿæˆå¤±è´¥: {str(e)}")
            self._mark_model_error(model_config)
            raise
    
    async def _call_openai(
        self,
        model_config: ModelConfig,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int
    ) -> str:
        """è°ƒç”¨OpenAI API"""
        
        # é…ç½®OpenAI
        openai.api_key = model_config.api_key
        if model_config.api_base:
            openai.api_base = model_config.api_base
        
        response = await openai.ChatCompletion.acreate(
            model=model_config.model_name,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=model_config.timeout
        )
        
        return response.choices[0].message.content
    
    async def _stream_openai(
        self,
        model_config: ModelConfig,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int
    ) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨OpenAI API"""
        
        openai.api_key = model_config.api_key
        if model_config.api_base:
            openai.api_base = model_config.api_base
        
        response = await openai.ChatCompletion.acreate(
            model=model_config.model_name,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=model_config.timeout,
            stream=True
        )
        
        async for chunk in response:
            if chunk.choices[0].delta.get("content"):
                yield chunk.choices[0].delta.content
    
    async def _call_azure_openai(
        self,
        model_config: ModelConfig,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int
    ) -> str:
        """è°ƒç”¨Azure OpenAI API"""
        
        # Azure OpenAIé…ç½®ç•¥æœ‰ä¸åŒ
        openai.api_type = "azure"
        openai.api_key = model_config.api_key
        openai.api_base = model_config.api_base
        openai.api_version = "2023-05-15"
        
        response = await openai.ChatCompletion.acreate(
            engine=model_config.model_name,  # Azureä½¿ç”¨engineè€Œä¸æ˜¯model
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=model_config.timeout
        )
        
        return response.choices[0].message.content
    
    async def _call_ollama(
        self,
        model_config: ModelConfig,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int
    ) -> str:
        """è°ƒç”¨Ollamaæœ¬åœ°æ¨¡å‹"""
        
        import aiohttp
        
        url = f"{model_config.api_base}/api/chat"
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        ollama_messages = [
            {"role": msg["role"], "content": msg["content"]}
            for msg in messages
        ]
        
        payload = {
            "model": model_config.model_name,
            "messages": ollama_messages,
            "stream": False,
            "options": {
                "temperature": temperature,
                "num_predict": max_tokens
            }
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                url,
                json=payload,
                timeout=aiohttp.ClientTimeout(total=model_config.timeout)
            ) as response:
                result = await response.json()
                return result["message"]["content"]
    
    async def _call_tongyi(
        self,
        model_config: ModelConfig,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int
    ) -> str:
        """è°ƒç”¨é€šä¹‰åƒé—® API"""
        
        import dashscope
        from dashscope import Generation
        
        # è®¾ç½® API Key
        dashscope.api_key = model_config.api_key
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        tongyi_messages = [
            {"role": msg["role"], "content": msg["content"]}
            for msg in messages
        ]
        
        # å®šä¹‰åŒæ­¥è°ƒç”¨å‡½æ•°
        def _sync_call():
            return Generation.call(
                model=model_config.model_name,
                messages=tongyi_messages,
                temperature=temperature,
                max_tokens=max_tokens,
                result_format='message'
            )
        
        # åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­è°ƒç”¨åŒæ­¥å‡½æ•°
        response = await asyncio.to_thread(_sync_call)
        
        if response.status_code == 200:
            return response.output.choices[0].message.content
        else:
            raise Exception(f"é€šä¹‰åƒé—® API è°ƒç”¨å¤±è´¥: {response.message}")
    
    async def _stream_tongyi(
        self,
        model_config: ModelConfig,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int
    ) -> AsyncGenerator[str, None]:
        """æµå¼è°ƒç”¨é€šä¹‰åƒé—® API"""
        
        import dashscope
        from dashscope import Generation
        
        # è®¾ç½® API Key
        dashscope.api_key = model_config.api_key
        
        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        tongyi_messages = [
            {"role": msg["role"], "content": msg["content"]}
            for msg in messages
        ]
        
        # å®šä¹‰åŒæ­¥æµå¼è°ƒç”¨å‡½æ•°ï¼Œè¿”å›ç”Ÿæˆå™¨
        def _get_stream_generator():
            return Generation.call(
                model=model_config.model_name,
                messages=tongyi_messages,
                temperature=temperature,
                max_tokens=max_tokens,
                result_format='message',
                stream=True,
                incremental_output=True
            )
        
        # åœ¨çº¿ç¨‹ä¸­è·å–ç”Ÿæˆå™¨å¯¹è±¡
        responses = await asyncio.to_thread(_get_stream_generator)
        
        # åœ¨å¾ªç¯ä¸­å¤„ç†å“åº”ï¼Œæ¯æ¬¡è¿­ä»£åè®©å‡ºæ§åˆ¶æƒ
        for response in responses:
            # è®©å‡ºæ§åˆ¶æƒï¼Œå…è®¸å…¶ä»–åç¨‹è¿è¡Œ
            await asyncio.sleep(0)
            
            if response.status_code == 200:
                if hasattr(response.output, 'choices') and len(response.output.choices) > 0:
                    choice = response.output.choices[0]
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¢é‡è¾“å‡º
                    if hasattr(choice, 'delta') and hasattr(choice.delta, 'content'):
                        content = choice.delta.content
                        if content:
                            yield content
                    # æˆ–è€…æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´æ¶ˆæ¯ï¼ˆæŸäº›æƒ…å†µä¸‹ï¼‰
                    elif hasattr(choice, 'message') and hasattr(choice.message, 'content'):
                        content = choice.message.content
                        if content:
                            yield content
            else:
                error_msg = getattr(response, 'message', 'æœªçŸ¥é”™è¯¯')
                raise Exception(f"é€šä¹‰åƒé—®æµå¼ API è°ƒç”¨å¤±è´¥: {error_msg}")
    
    def get_model_stats(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        
        stats = {}
        
        for task_type, models in self.models.items():
            stats[task_type.value] = []
            
            for model in models:
                model_key = f"{model.provider}:{model.model_name}"
                
                # è®¡ç®—æœ€è¿‘ä¸€åˆ†é’Ÿçš„è°ƒç”¨æ¬¡æ•°
                call_count = len(self.call_history.get(model_key, []))
                
                stats[task_type.value].append({
                    "provider": model.provider.value,
                    "model_name": model.model_name,
                    "priority": model.priority,
                    "is_available": model.is_available,
                    "error_count": model.error_count,
                    "calls_last_minute": call_count,
                    "rate_limit": model.rate_limit_per_minute
                })
        
        return stats


# å…¨å±€æ¨¡å‹è°ƒåº¦å™¨å®ä¾‹
model_orchestrator = ModelOrchestrator()

